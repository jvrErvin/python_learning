{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees and Random Forests\n",
    "\n",
    "## Part 1: Introduction to Decision Trees\n",
    "\n",
    "- https://www.youtube.com/watch?v=_L39rN6gz7Y\n",
    "- https://www.ibm.com/topics/decision-trees\n",
    "- https://www.geeksforgeeks.org/decision-tree/\n",
    "\n",
    "### Theory\n",
    "\n",
    "#### 1. What is a Decision Tree?\n",
    "A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. It is a tree-like model where each internal node represents a test on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label (in classification) or a continuous value (in regression).\n",
    "\n",
    "#### 2. How Decision Trees Work\n",
    "The fundamental idea behind decision trees is to split the dataset into subsets based on the value of input features, with the goal of maximizing the separation of classes (in classification) or minimizing prediction error (in regression).\n",
    "\n",
    "#### 3. Goal of Minimizing Entropy in Decision Trees\n",
    "##### 3.1. Entropy and Information Gain\n",
    "\n",
    "- **Entropy** is a measure of impurity or randomness in the data. In the context of decision trees, we want to partition the data in such a way that the resulting subsets are as pure as possible (i.e., they contain instances of only one class or are as homogeneous as possible).\n",
    "- **Information Gain** quantifies the reduction in entropy achieved by splitting the dataset based on a particular attribute. It is calculated as the difference between the entropy of the dataset before the split and the weighted sum of the entropies of the subsets after the split.\n",
    "\n",
    "##### 3.2. Minimizing Entropy\n",
    "\n",
    "- When building a decision tree, we aim to select the attribute that provides the highest information gain, which effectively means selecting the attribute that results in the greatest reduction in entropy.\n",
    "- By iteratively selecting attributes that minimize entropy, the decision tree algorithm creates partitions that are increasingly pure, leading to better classification performance.\n",
    "\n",
    "\n",
    "\n",
    "- **Entropy and Information Gain**\n",
    "  - **Entropy** measures the amount of uncertainty or impurity in a dataset. For a binary classification, the entropy $H$ of a set $S$ with $p$ positive examples and $n$ negative examples is given by:\n",
    "    \n",
    "    $\n",
    "    H(S) = -p \\log_2(p) - n \\log_2(n)\n",
    "    $\n",
    "    \n",
    "  - **Information Gain** measures the reduction in entropy or impurity achieved by partitioning the set $S$ into subsets based on an attribute $A$. The information gain $IG$ of an attribute $A$ is:\n",
    "    \n",
    "    $\n",
    "    IG(S, A) = H(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} H(S_v)\n",
    "    $\n",
    "    \n",
    "    where $S_v$ is the subset of $S$ for which attribute $A$ has value $v$.\n",
    "\n",
    "- **Gini Impurity**\n",
    "  - **Gini Impurity** is another measure of impurity or uncertainty used by the CART (Classification and Regression Tree) algorithm. The Gini impurity $G$ of a set $S$ with $p$ positive examples and $n$ negative examples is:\n",
    "    \n",
    "    $\n",
    "    G(S) = 1 - (p^2 + n^2)\n",
    "    $\n",
    "\n",
    "#### 4. Advantages and Disadvantages\n",
    "- **Advantages**:\n",
    "  - **Interpretability**: Decision trees are easy to understand and interpret. The decision rules can be easily visualized.\n",
    "  - **Non-linearity**: Decision trees can capture non-linear relationships between features.\n",
    "  - **Feature Importance**: They can measure the importance of different features in the prediction task.\n",
    "  \n",
    "- **Disadvantages**:\n",
    "  - **Overfitting**: Decision trees can easily overfit the training data, especially if they are deep.\n",
    "  - **Instability**: Small variations in the data can lead to completely different trees.\n",
    "\n",
    "#### 5. Example Calculation: Entropy and Information Gain\n",
    "\n",
    "Consider a dataset with the following distribution of classes:\n",
    "\n",
    "| Feature | Class |\n",
    "|---------|-------|\n",
    "| Sunny   | Yes   |\n",
    "| Sunny   | Yes   |\n",
    "| Rainy   | No    |\n",
    "| Sunny   | Yes   |\n",
    "| Rainy   | No    |\n",
    "\n",
    "- **Entropy of the entire dataset \\( S \\)**:\n",
    "  - Total instances: 5\n",
    "  - Positive instances (Yes): 3\n",
    "  - Negative instances (No): 2\n",
    "  \n",
    "  $\n",
    "  H(S) = -\\left(\\frac{3}{5}\\right) \\log_2\\left(\\frac{3}{5}\\right) - \\left(\\frac{2}{5}\\right) \\log_2\\left(\\frac{2}{5}\\right)\n",
    "  = -0.6 \\log_2(0.6) - 0.4 \\log_2(0.4) \\approx 0.971\n",
    "  $\n",
    "\n",
    "- **Information Gain for the feature \"Weather\"**:\n",
    "  - Split the dataset based on \"Weather\":\n",
    "    - For \"Sunny\": 3 instances, all \"Yes\"\n",
    "    - For \"Rainy\": 2 instances, all \"No\"\n",
    "  - Entropy for \"Sunny\" subset:\n",
    "  - \n",
    "    $\n",
    "    H(S_{\\text{Sunny}}) = -\\left(\\frac{3}{3}\\right) \\log_2\\left(\\frac{3}{3}\\right) = 0\n",
    "    $\n",
    "\n",
    "  - Entropy for \"Rainy\" subset:\n",
    "  - \n",
    "    $\n",
    "    H(S_{\\text{Rainy}}) = -\\left(\\frac{2}{2}\\right) \\log_2\\left(\\frac{2}{2}\\right) = 0\n",
    "    $\n",
    "\n",
    "  - Weighted average entropy after the split:\n",
    "  - \n",
    "    $\n",
    "    H(S, \\text{Weather}) = \\left(\\frac{3}{5}\\right) H(S_{\\text{Sunny}}) + \\left(\\frac{2}{5}\\right) H(S_{\\text{Rainy}}) = 0\n",
    "    $\n",
    "\n",
    "  - Information Gain:\n",
    "  - \n",
    "    $\n",
    "    IG(S, \\text{Weather}) = H(S) - H(S, \\text{Weather}) = 0.971 - 0 = 0.971\n",
    "    $\n",
    "\n",
    "This example shows how information gain helps in selecting the best feature for splitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the Decision Tree model\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Decision Tree\n",
    "plt.figure(figsize=(20,20))\n",
    "plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the decision boundaries of the tree\n",
    "But for this, let's only use the first two features from the dataset, so we can plot the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]  # Use only the first two features for easy visualization\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the Decision Tree model\n",
    "clf = DecisionTreeClassifier(max_depth=20)\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Decision Tree\n",
    "plt.figure(figsize=(20,20))\n",
    "plot_tree(clf, feature_names=iris.feature_names[:2], class_names=iris.target_names, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mesh grid\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                     np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "# Predict classes for each point in the mesh grid\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor='k')\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1])\n",
    "plt.title('Decision Tree Decision Boundaries')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Introduction to Random Forests\n",
    "\n",
    "### Theory\n",
    "- https://www.youtube.com/watch?v=v6VJ2RO66Ag\n",
    "- https://www.geeksforgeeks.org/random-forest-algorithm-in-machine-learning/\n",
    "\n",
    "#### 1. What is a Random Forest?\n",
    "A Random Forest is an ensemble learning method that combines multiple decision trees to produce a more robust and accurate model. It is used for both classification and regression tasks. The main idea is to aggregate the predictions of many individual decision trees to reduce overfitting and improve generalization.\n",
    "\n",
    "#### 2. How Random Forests Work\n",
    "- **Bootstrapping**: Random forests use a technique called bootstrapping to create multiple subsets of the training data. Each decision tree in the forest is trained on a different bootstrap sample, which is created by randomly sampling the training data with replacement.\n",
    "- **Feature Randomness**: When splitting a node, each tree in a random forest considers a random subset of the features, adding further diversity among the trees.\n",
    "- **Aggregation**: The final prediction of the random forest is obtained by aggregating the predictions of all individual trees (e.g., by majority vote for classification or averaging for regression).\n",
    "\n",
    "#### 3. Advantages and Disadvantages\n",
    "- **Advantages**:\n",
    "  - **Robustness**: Random forests are less prone to overfitting compared to individual decision trees due to the ensemble approach.\n",
    "  - **Higher Accuracy**: They often achieve higher accuracy by reducing the variance of the model.\n",
    "  - **Feature Importance**: Random forests can provide estimates of feature importance.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - **Less Interpretability**: While individual decision trees are interpretable, random forests, being an ensemble of many trees, are less interpretable.\n",
    "  - **Computationally Intensive**: Training and predicting with random forests can be computationally intensive, especially with a large number of trees.\n",
    "\n",
    "### Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the Random Forest model\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf = rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "print(f'Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importances\n",
    "feature_importances = rf_clf.feature_importances_\n",
    "features = iris.feature_names\n",
    "feature_importances_df = pd.DataFrame({'feature': features, 'importance': feature_importances})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and plot\n",
    "feature_importances_df = feature_importances_df.sort_values(by='importance', ascending=False)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importances_df)\n",
    "plt.title('Feature Importances in Random Forest')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
